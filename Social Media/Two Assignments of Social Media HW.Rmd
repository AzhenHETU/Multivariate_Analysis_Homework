---
title: "Social Media"
author: "Yuefei Chen"
date: "2024-03-29"
output: html_document
---
# Loading the Dataset
# In the dataset of "Social_media_cleaned.csv", the data is cleaned and every time stamp data has been transformed into numeric value data. These values are in the new columns "XXX_value". XXX means the Apps we use. Mean value of the data is also a kind of numeric data, which are shown in the last line. Additionally, N/A value has been replaced by 0.00. The point of my mean value of social media is in the line 23.  
```{r}
library(readr)
APP_data <- read_csv("Social Media_cleaned.csv")
APP_data <- APP_data[c(1:22), c(1:2, 4:5, 7:8, 10:11, 13:14, 16:17, 19:20, 22:23, 25:33)]
str(APP_data)
```

# Part I: Calculate the MVA distance of your social media usage and the class average

```{r}
MVA_data <- APP_data[c(1:22), c(3,5,7,9,11,13,15,17,19,20,21,25)]
cov_matrix <- cov(MVA_data)
mean_vector <- colMeans(MVA_data)
#pseudo_inv_cov <- solve(cov_matrix + diag(rep(1e-5, ncol(data))))
mahalanobis_distances <- mahalanobis(MVA_data, center = mean_vector, cov = cov_matrix)
mahalanobis_distances[22]
```

The MVA distance of my social media usage and the class average is 7.126339.

# Part II: Social Media Data - Midterm Prep

## PCA Analysis

```{R}
APP_pca <- prcomp(MVA_data[,-c(9:12)],scale=TRUE)
APP_pca
summary(APP_pca)

(eigen_rent <- APP_pca$sdev^2)
names(eigen_rent) <- paste("PC",1:8,sep="")
eigen_rent
plot(eigen_rent, xlab = "Component number", ylab = "Component variance", type = "l", main = "Scree diagram")
plot(log(eigen_rent), xlab = "Component number",ylab = "log(Component variance)", type="l",main = "Log(eigenvalue) diagram")
```
This model tells us these social media usage variables can be transformed into three components. 




```{R}
APP_pca_id <- cbind(APP_data[1:22,23],APP_pca$x)
APP_pca_id
var.test(PC1~APP_data$`Tired waking up in morning`,data=APP_pca_id)
var.test(PC2~APP_data$`Tired waking up in morning`,data=APP_pca_id)
var.test(PC3~APP_data$`Tired waking up in morning`,data=APP_pca_id)
var.test(PC4~APP_data$`Tired waking up in morning`,data=APP_pca_id)
var.test(PC5~APP_data$`Tired waking up in morning`,data=APP_pca_id)
var.test(PC6~APP_data$`Tired waking up in morning`,data=APP_pca_id)
var.test(PC7~APP_data$`Tired waking up in morning`,data=APP_pca_id)
var.test(PC8~APP_data$`Tired waking up in morning`,data=APP_pca_id)
pairs(APP_pca$x[,1:8], ylim = c(-6,4),xlim = c(-6,4),panel=function(x,y,...){text(x,y,APP_pca_id$`Tired waking up in morning`)})
```
```{R}
library(factoextra)
library(FactoMineR)
library(ggfortify)
library(psych)
library(corrplot)
library(devtools)

# Correlation
pairs.panels(MVA_data[,-c(9:12)],
             gap = 0,
             bg = c("red", "blue")[APP_data$`Tired waking up in morning`],
             pch=21)




pairs.panels(APP_pca$x,
             gap=0,
             bg = c("red", "blue")[APP_data$`Tired waking up in morning`],
             pch=21)

```
## Cluster Analysis
In this case, since the dataset is not large and we do not know how many cluster we need. Hierarchical cluster analysis will be used in this model.
```{R}
library(cluster)
library(readr)
library(factoextra)
library(magrittr)
library(NbClust)
matstd.APP <- scale(MVA_data)
dist.APP <- dist(matstd.APP, method="euclidean")
clusAPP.nn <- hclust(dist.APP, method = "single")
plot(as.dendrogram(clusAPP.nn),ylab="Distance between classmates",ylim=c(0,6),main="Dendrogram. social media usage")
plot(as.dendrogram(clusAPP.nn), xlab= "Distance between classmates", xlim=c(6,0), horiz = TRUE,main="Dendrogram. social media usage")
(agn.APP <- agnes(MVA_data, metric="euclidean", stand=TRUE, method = "single"))
agn.APP$merge
plot(as.dendrogram(agn.APP), xlab= "Distance between Countries",xlim=c(8,0), horiz = TRUE,main="Dendrogram \n social media usage")
plot(agn.APP, which.plots=1)
plot(agn.APP, which.plots=2)
plot(agn.APP, which.plots=3)



```







#factor analysis

```{R}
fit.pc <- principal(MVA_data, nfactors=4, rotate="varimax")
fit.pc
round(fit.pc$values, 3)
fit.pc$loadings
fa.parallel(MVA_data)
fa.plot(fit.pc)
fa.diagram(fit.pc)
vss(MVA_data)
fit.pc1 <- principal(MVA_data, nfactors=2, rotate="varimax")
round(fit.pc$values, 3)

```
```{R}
(eigen_APP_vars <- round(APP_pca$sdev^2,3))
names(eigen_APP_vars) <- paste("PC",1:8,sep="")
sumlambdas <- sum(eigen_APP_vars)
propvar <- round(eigen_APP_vars/sumlambdas,2)
cumvar_APP_vars <- cumsum(propvar)
matlambdas <- rbind(eigen_APP_vars,propvar,cumvar_APP_vars)
rownames(matlambdas) <- c("Eigenvalues","Prop. variance","Cum. prop. variance")
eigvec.emp <- APP_pca$rotation
pcafactors.emp <- eigvec.emp[,1:2]
unrot.fact.emp <- sweep(pcafactors.emp,MARGIN=2,APP_pca$sdev[1:2],`*`)
communalities.emp <- rowSums(unrot.fact.emp^2)
communalities.emp
rot.fact.emp <- varimax(unrot.fact.emp)
rot.fact.emp
fact.load.emp <- rot.fact.emp$loadings[1:6,1:2]
fact.load.emp
scale.emp <- scale(MVA_data[,-c(9:12)])
scale.emp
```