---
title: "final project social media Yuefei Chen"
author: "Yuefei Chen"
date: "2024-04-29"
output:
  html_document: default
  pdf_document: default
---
# final project on Social Media

## Question 1. Explain the data collection process. (10 points)
In the dataset of "Social_media_cleaned.csv", the data is cleaned and every time stamp data has been transformed into numeric value data. These values are in the new columns "XXX_value". XXX means the Apps we use. Mean value of the data is also a kind of numeric data, which are shown in the last line. Additionally, N/A value has been replaced by 0.00. The point of my mean value of social media is in the line 23.  
```{R}
library(readr)
APP_data <- read_csv("Dataset/Social Media_cleaned.csv")
APP_data <- APP_data[c(1:22), c(1:2, 4:5, 7:8, 10:11, 13:14, 16:17, 19:20, 22:23, 25:33)]
str(APP_data)
```



```{R}
summary(APP_data)
```


## Question 2. Exploratory Data Analysis and Visualizations (50 points)
# Part I: Calculate the MVA distance of your social media usage and the class average

```{r}
MVA_data <- APP_data[c(1:22), c(3,5,7,9,11,13,15,17,19,20,21,25)]
cov_matrix <- cov(MVA_data)
mean_vector <- colMeans(MVA_data)
mahalanobis_distances <- mahalanobis(MVA_data, center = mean_vector, cov = cov_matrix)
mahalanobis_distances[22]
```

The MVA distance of my social media usage and the class average is 7.126339.

# Part II: Social Media Data


## PCA Analysis
In the PCA model tells us these social media usage variables can be transformed into three components. Since after component 3 point, the curve decreasing becomes slow, and additionally, only first 3 components' variance are larger than 1, 3 principal components will be selected as PCA analysis model. When we test whether PCs will affect the "Tired_waking_up_in_morning". The p-value shows the hypothesis is not significant, so we cannot conclude these usage will affect classmates feeling when waking_up_in_morning.
In this cluster analysis, since the dataset is not large and we do not know how many cluster we need. Hierarchical cluster analysis will be used in this model. These points will be clustered into two clusters. The cluster one is {9, 15, 20}. The cluster 2 is {1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 16, 17, 18, 19, 21, 22}
In this factor analysis model, four factors are ideal for the dataset. That is because from the scree plot there are significant decrease of the line before the factor is 4. After factor = 4, the change of the line is not significant. And after factor = 4, the data point is under the eigenvalue line. Additionally, from the chart of Very Simple Structure, factor = 4 line has good performance in fit. In component analysis, the factor loading between PC1 and Instagram, Snapchat, WhatsApp/Wechat are 0.9, 0.8, 0.6. The factor loading between PC2 and Twitter, OTT are 0.9, 0.7. The factor loading between PC3 and Linkedin, Youtube are 0.8, 0.8. The factor loading between PC4 and Reddit is 1.
```{R}
APP_pca <- prcomp(MVA_data[,-c(9:12)],scale=TRUE)
APP_pca
summary(APP_pca)

(eigen_rent <- APP_pca$sdev^2)
names(eigen_rent) <- paste("PC",1:8,sep="")
eigen_rent
plot(eigen_rent, xlab = "Component number", ylab = "Component variance", type = "l", main = "Scree diagram")
plot(log(eigen_rent), xlab = "Component number",ylab = "log(Component variance)", type="l",main = "Log(eigenvalue) diagram")
```






```{R}
APP_pca_id <- cbind(APP_data[1:22,23],APP_pca$x)
APP_pca_id
var.test(PC1~APP_data$`Tired_waking_up_in_morning`,data=APP_pca_id)
var.test(PC2~APP_data$`Tired_waking_up_in_morning`,data=APP_pca_id)
var.test(PC3~APP_data$`Tired_waking_up_in_morning`,data=APP_pca_id)
var.test(PC4~APP_data$`Tired_waking_up_in_morning`,data=APP_pca_id)
var.test(PC5~APP_data$`Tired_waking_up_in_morning`,data=APP_pca_id)
var.test(PC6~APP_data$`Tired_waking_up_in_morning`,data=APP_pca_id)
var.test(PC7~APP_data$`Tired_waking_up_in_morning`,data=APP_pca_id)
var.test(PC8~APP_data$`Tired_waking_up_in_morning`,data=APP_pca_id)
pairs(APP_pca$x[,1:8], ylim = c(-6,4),xlim = c(-6,4),panel=function(x,y,...){text(x,y,APP_pca_id$`Tired_waking_up_in_morning`)})
```


## Cluster Analysis

```{R}
library(cluster)
library(readr)
library(factoextra)
library(magrittr)
library(NbClust)
matstd.APP <- scale(MVA_data)
dist.APP <- dist(matstd.APP, method="euclidean")
clusAPP.nn <- hclust(dist.APP, method = "single")
plot(as.dendrogram(clusAPP.nn),ylab="Distance between classmates",ylim=c(0,6),main="Dendrogram. social media usage")
plot(as.dendrogram(clusAPP.nn), xlab= "Distance between classmates", xlim=c(6,0), horiz = TRUE,main="Dendrogram. social media usage")
(agn.APP <- agnes(MVA_data, metric="euclidean", stand=TRUE, method = "single"))
agn.APP$merge
plot(as.dendrogram(agn.APP), xlab= "Distance between Countries",xlim=c(8,0), horiz = TRUE,main="Dendrogram \n social media usage")




plot(agn.APP, which.plots=1)
plot(agn.APP, which.plots=2)
plot(agn.APP, which.plots=3)
```
#factor analysis

```{R}
library(psych)
fit.pc <- principal(MVA_data[,-c(9:12)], nfactors=4, rotate="varimax")
fit.pc
round(fit.pc$values, 3)
fit.pc$loadings
fa.parallel(MVA_data)
fa.plot(fit.pc)
fa.diagram(fit.pc)
vss(MVA_data)
```

## Question 3. Application of different MVA models (10 points) 
### Multiregression model
#### Model development
Running the following code, we build a multiple regression model based on rent house data. Its independent variables "Instagram_value", "Linkedin_value", "Snapchat_value", "Twitter_value", "Whatsapp_Wechat_value", "Youtube_value", "OTT_Netflix_Hulu_Prime_video_value", "Reddit_value", "job_interview_calls", "networking_done_with_coffee_chats", "learning_done_in_terms_of_items_created". The dependent variable is "felt_the_entire_week". 

```{R}
library(readr)
APP_data <- read_csv("Dataset/Social Media_cleaned.csv")
APP_data <- APP_data[c(1:22), c(1:2, 4:5, 7:8, 10:11, 13:14, 16:17, 19:20, 22:23, 25:33)]
str(APP_data)
```
	
```{R}
fit <- lm(felt_the_entire_week ~ Instagram_value + Linkedin_value + Snapchat_value + Twitter_value + Whatsapp_Wechat_value + Youtube_value + OTT_Netflix_Hulu_Prime_video_value + Reddit_value + job_interview_calls + networking_done_with_coffee_chats + learning_done_in_terms_of_items_created, data=APP_data)
fit
```


#### Model Acceptance
In the summary of the model, we focus on R squared value, coefficients, and P-value of each coefficient. The R-squared value is 0.4434 and Adjust R-squared value is -0.1688. It shows there is a low proportion of variance in the dependent variable can be explained by the independent variables. Therefore, we use stepAIC to find an optimal model.
```{R}
summary(fit)
coefficients(fit)
```
```{R}
library(MASS)
step <- stepAIC(fit, direction="both")
fit2 <- lm(felt_the_entire_week ~ Instagram_value + Whatsapp_Wechat_value + 
    job_interview_calls, data = APP_data)

```




#### Residual Analysis
QQ plot is used in these residual analysis. We can conclude that most of residual points are located in a straight line. It satisfies normal distribution. 

```{R}
confint(fit2,level=0.95)
fitted(fit2)
residuals(fit2)
```

```{R}
library(car)
qqPlot(fit2, main="QQ Plot")
```

#### Prediction
We set a data point with Instagram_value = 5, Whatsapp_Wechat_value = 5 and, job_interview_calls = 0, then the feeling score of the entire week we predict is approximate to 3
```{R}
predict.lm(fit2, data.frame(Instagram_value = 5, Whatsapp_Wechat_value = 5, job_interview_calls = 0))
```


#### Model Accuracy
The accuracy is based on summary of the model and we also calculate the MSE and RMSE for the model. The MSE is 0.3846858 and RMSE is 0.6202304.
```{R}
summary(fit2)
predictions <- predict(fit2, APP_data)
mse <- mean((APP_data$felt_the_entire_week - predictions)^2)
rmse <- sqrt(mse)
cat("MSE: ", mse, "\n")
cat("RMSE: ", rmse, "\n")

```

### lda model



#### Model development
Running the following code, we build a linear discriminant analysis model to classify social media data. Its independent variables “Instagram_value”, “Linkedin_value”, “Snapchat_value”, “Twitter_value”, “Whatsapp_Wechat_value”, “Youtube_value”, “OTT_Netflix_Hulu_Prime_video_value”, “Reddit_value”, “job_interview_calls”, “networking_done_with_coffee_chats”, “learning_done_in_terms_of_items_created”. The dependent variable is “Tired_waking_up_in_morning”.

```{r}
library(MASS)
library(ggplot2)
library(memisc)
library(ROCR)
library(dplyr)
library(klaR)
library(readr)
APP_data <- read_csv("Dataset/Social Media_cleaned.csv")
APP_data <- APP_data[c(1:22), c(1:2, 4:5, 7:8, 10:11, 13:14, 16:17, 19:20, 22:23, 25:33)]
str(APP_data)
```

```{R}
APP_data$Tired_waking_up_in_morning <- as.factor(APP_data$Tired_waking_up_in_morning)
r <- lda(formula = Tired_waking_up_in_morning ~ Instagram_value + Linkedin_value + Snapchat_value + Twitter_value + Whatsapp_Wechat_value + Youtube_value + OTT_Netflix_Hulu_Prime_video_value + Reddit_value + networking_done_with_coffee_chats + learning_done_in_terms_of_items_created, data = APP_data)
head(r$class)
summary(r)
```
### Model Acceptance
In this model, we can see that the first linear discriminant explains all the between-group variance in the house data. Therefore, the model can be used to analyze the house data.
```{R}
r$svd
(prop = r$svd^2/sum(r$svd^2))
```

### Residual Analysis
Since this model is a classification model, we focus on the posterior value of the model. The following code is to train the new model r3 and the model is used to test the model and display the predicted result and posterior probability. The plots of r1 and r3 shows how the model distinguishes between different furniture categories on training data

```{R}
r2 <- lda(formula = Tired_waking_up_in_morning ~ Instagram_value + Linkedin_value + Snapchat_value + Twitter_value + Whatsapp_Wechat_value + Youtube_value + OTT_Netflix_Hulu_Prime_video_value + Reddit_value + networking_done_with_coffee_chats + learning_done_in_terms_of_items_created, data = APP_data, CV = TRUE)
head(r2$posterior, 3)
plot(r)
train <- sample(22, 10)
r3 <- lda(Tired_waking_up_in_morning ~ Instagram_value + Linkedin_value + Snapchat_value + Twitter_value + Whatsapp_Wechat_value + Youtube_value + OTT_Netflix_Hulu_Prime_video_value + Reddit_value + networking_done_with_coffee_chats + learning_done_in_terms_of_items_created, 
          APP_data,
          prior = c(1,1)/2,
          subset = train)
plot(r3)
plda = predict(object = r3, # predictions
               newdata = APP_data[-train, ])
head(plda$class)
head(plda$posterior, 6) # posterior prob.
head(plda$x, 3)
```



### Prediction
The data will be predicted in the model and the predicted first linear discriminant scores of the  are as follows.
```{R}
r <- lda(Tired_waking_up_in_morning ~ Instagram_value + Linkedin_value + Snapchat_value + Twitter_value + Whatsapp_Wechat_value + Youtube_value + OTT_Netflix_Hulu_Prime_video_value + Reddit_value + job_interview_calls + networking_done_with_coffee_chats + learning_done_in_terms_of_items_created, 
          APP_data,
          prior = c(1,1)/2,)
prop.lda = r$svd^2/sum(r$svd^2)
plda <- predict(object = r,
                newdata = APP_data)
dataset = data.frame(furniture = APP_data[,"Tired_waking_up_in_morning"],lda = plda$x)
dataset$LD1
```

### Model Accuracy
To observe the performance of the model, the test set is used to approximate accuracy. 

```{R}
set.seed(101)
sample_n(APP_data,10)
training_sample <- sample(c(TRUE, FALSE), nrow(APP_data), replace = T, prob = c(0.75,0.25))
train <- APP_data[training_sample, ]
test <- APP_data[!training_sample, ]
lda.waking <- lda(Tired_waking_up_in_morning ~ Instagram_value + Linkedin_value + Snapchat_value + Twitter_value + Whatsapp_Wechat_value + Youtube_value + OTT_Netflix_Hulu_Prime_video_value + Reddit_value + job_interview_calls + networking_done_with_coffee_chats + learning_done_in_terms_of_items_created, train)
plot(lda.waking, col = as.integer(train$Tired_waking_up_in_morning))
# Sometime bell curves are better
plot(lda.waking, dimen = 1, type = "b")



lda.train <- predict(lda.waking)
train$lda <- lda.train$class
table(train$lda,train$Tired_waking_up_in_morning)
# running accuracy on the training set shows how good the model is. It is not an indication of "true" accuracy. We will use the test set to approximate accuracy
lda.test <- predict(lda.waking,test)
test$lda <- lda.test$class
table(test$lda,test$Tired_waking_up_in_morning)
```

## Question 4 Model Insights  (10 points)
We uses clustering techniques, indicating distinct groups or patterns within the data, which could be crucial for understanding different user behaviors and tailoring specific interventions or marketing strategies. The hierarchical clustering results underscore the diversity in social media usage among individuals, which might correlate with their professional networking activities and job-related outcomes.





## Problem 5 Learnings and Takeaways (20 points)
Learnings:
The most important thing learned is diversity in model evaluation: By evaluating model performance using different statistical metrics such as R-squared, adjusted R-squared, and F-statistics, you can gain a comprehensive understanding of the model's explanatory and predictive power. This helps in selecting the most suitable model for prediction or classification.
Take aways:
The necessity for careful selection of variables in model building, as shown by the stepwise regression outcomes.
The application of clustering methods can reveal hidden patterns and segments within the data, which are crucial for targeted marketing and investment strategies in social media usage.
PCA and factor analysis are powerful tools for reducing complexity in data, allowing easier interpretation and visualization.